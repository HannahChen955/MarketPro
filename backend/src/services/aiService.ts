import { PrismaClient } from '@prisma/client';
import { AISecurityService, SecurityCheckResult } from './aiSecurityService';
import * as crypto from 'crypto';

// AI Provider Types
export type AIProvider = 'qwen' | 'openai' | 'mock';

export interface AIRequest {
  prompt: string;
  userId: string;
  requestType: 'generation' | 'analysis' | 'chat' | 'summary';
  maxTokens?: number;
  temperature?: number;
  projectId?: string;
  taskId?: string;
  context?: Record<string, any>;
}

export interface AIResponse {
  content: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model: string;
  provider: AIProvider;
  cached: boolean;
  filteredInput?: boolean;
}

export interface AIProviderConfig {
  name: AIProvider;
  model: string;
  apiKey: string;
  endpoint?: string;
  maxTokens: number;
  enabled: boolean;
  priority: number;
}

export interface AIAnalysisResult {
  extractedContent: {
    sections: string[];
    keyPoints: string[];
    charts: number;
    images: number;
    tables: number;
  };
  designPatterns: {
    colorScheme: string[];
    fonts: string[];
    layout: string;
    style: string;
  };
  suggestedTemplate: {
    reportType: string;
    estimatedTime: string;
    requiredFields: string[];
    outputFormat: string[];
  };
}

export interface ReportGenerationConfig {
  projectName: string;
  projectDescription?: string;
  reportType: string;
  analysisArea: string;
  timeRange: string;
  additionalRequirements?: string;
  outputFormat: 'pptx' | 'pdf' | 'html';
}

export interface GeneratedContent {
  title: string;
  sections: GeneratedSection[];
  metadata: {
    generatedAt: string;
    model: string;
    confidence: number;
    wordCount: number;
  };
}

export interface GeneratedSection {
  id: string;
  title: string;
  content: string;
  type: 'text' | 'chart' | 'table' | 'image';
  data?: any;
}

export class AIService {
  private prisma: PrismaClient;
  private securityService: AISecurityService;
  private providers: Map<AIProvider, AIProviderConfig>;
  private responseCache: Map<string, AIResponse>;
  private qwenApiKey: string;
  private openaiApiKey: string;
  private baseURL: string;

  constructor() {
    this.prisma = new PrismaClient();
    this.securityService = new AISecurityService();
    this.providers = new Map();
    this.responseCache = new Map();
    this.qwenApiKey = process.env.QWEN_API_KEY || '';
    this.openaiApiKey = process.env.OPENAI_API_KEY || '';
    this.baseURL = process.env.QWEN_BASE_URL || 'https://dashscope.aliyuncs.com/api/v1';

    this.initializeProviders();

    if (!this.qwenApiKey && !this.openaiApiKey) {
      console.warn('âš ï¸  No AI API keys found. AI features will be simulated.');
    }
  }

  /**
   * åˆå§‹åŒ– AI æä¾›å•†é…ç½®
   */
  private initializeProviders() {
    // é€šä¹‰åƒé—®é…ç½®
    if (this.qwenApiKey) {
      this.providers.set('qwen', {
        name: 'qwen',
        model: 'qwen-turbo',
        apiKey: this.qwenApiKey,
        endpoint: 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation',
        maxTokens: 8000,
        enabled: true,
        priority: 1
      });
    }

    // OpenAI é…ç½®
    if (this.openaiApiKey) {
      this.providers.set('openai', {
        name: 'openai',
        model: 'gpt-3.5-turbo',
        apiKey: this.openaiApiKey,
        endpoint: 'https://api.openai.com/v1/chat/completions',
        maxTokens: 4000,
        enabled: true,
        priority: 2
      });
    }

    // Mock æä¾›å•†ï¼ˆå¼€å‘æµ‹è¯•ç”¨ï¼‰
    this.providers.set('mock', {
      name: 'mock',
      model: 'mock-model',
      apiKey: 'mock-key',
      maxTokens: 1000,
      enabled: true,
      priority: 10
    });
  }

  /**
   * ä¸»è¦çš„ AI è¯·æ±‚å¤„ç†æ–¹æ³•
   */
  async generateResponse(request: AIRequest): Promise<AIResponse> {
    const { prompt, userId, requestType } = request;

    try {
      // 1. å®‰å…¨æ£€æŸ¥
      const securityCheck = await this.securityService.checkInputSecurity(prompt, userId);
      if (!securityCheck.isSafe) {
        throw new Error(`å®‰å…¨æ£€æŸ¥å¤±è´¥: ${securityCheck.issues.join(', ')}`);
      }

      // 2. æ£€æŸ¥æœåŠ¡é™çº§
      const degradationCheck = await this.securityService.checkServiceDegradation();
      if (degradationCheck.shouldDegrade) {
        return this.handleServiceDegradation(request, degradationCheck);
      }

      // 3. æ£€æŸ¥ç¼“å­˜
      const cacheKey = this.generateCacheKey(request);
      const cachedResponse = this.responseCache.get(cacheKey);
      if (cachedResponse) {
        console.log(`ç¼“å­˜å‘½ä¸­: ${cacheKey}`);
        return { ...cachedResponse, cached: true };
      }

      // 4. é€‰æ‹©å¯ç”¨çš„ AI æä¾›å•†
      const provider = this.selectProvider(request);
      if (!provider) {
        throw new Error('æ²¡æœ‰å¯ç”¨çš„ AI æä¾›å•†');
      }

      // 5. è°ƒç”¨ AI æœåŠ¡
      const response = await this.callAIProvider(
        provider,
        securityCheck.filteredContent || prompt,
        request
      );

      // 6. è®°å½•ä½¿ç”¨ç»Ÿè®¡
      await this.securityService.trackAICost({
        userId,
        model: response.model,
        promptTokens: response.usage.promptTokens,
        completionTokens: response.usage.completionTokens,
        requestType,
        successful: true
      });

      // 7. ç¼“å­˜å“åº”
      this.responseCache.set(cacheKey, response);

      // 8. æ¸…ç†è¿‡æœŸç¼“å­˜
      this.cleanupCache();

      return {
        ...response,
        cached: false,
        filteredInput: !!securityCheck.filteredContent
      };

    } catch (error) {
      console.error('AI è¯·æ±‚å¤±è´¥:', error);

      // è®°å½•å¤±è´¥çš„ä½¿ç”¨ç»Ÿè®¡
      await this.securityService.trackAICost({
        userId,
        model: 'unknown',
        promptTokens: prompt.length / 4,
        completionTokens: 0,
        requestType,
        successful: false
      });

      throw error;
    }
  }

  /**
   * åˆ†æä¸Šä¼ çš„æ–‡ä»¶å†…å®¹
   */
  async analyzeFile(fileBuffer: Buffer, mimeType: string, fileName: string): Promise<AIAnalysisResult> {
    try {
      console.log(`ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: ${fileName}`);

      // æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
      let extractedText = '';

      if (mimeType === 'application/pdf') {
        extractedText = await this.extractPdfContent(fileBuffer);
      } else if (mimeType.includes('presentation')) {
        extractedText = await this.extractPptxContent(fileBuffer);
      } else {
        throw new Error('ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹');
      }

      // ä½¿ç”¨ AI åˆ†æå†…å®¹
      const analysisResult = await this.performContentAnalysis(extractedText, fileName);

      console.log(`âœ… æ–‡ä»¶åˆ†æå®Œæˆ: ${fileName}`);
      return analysisResult;

    } catch (error) {
      console.error('æ–‡ä»¶åˆ†æå¤±è´¥:', error);

      // è¿”å›æ¨¡æ‹Ÿåˆ†æç»“æœ
      return this.generateMockAnalysisResult(fileName);
    }
  }

  /**
   * ç”ŸæˆæŠ¥å‘Šå†…å®¹
   */
  async generateReport(config: ReportGenerationConfig): Promise<GeneratedContent> {
    try {
      console.log(`ğŸ“ å¼€å§‹ç”ŸæˆæŠ¥å‘Š: ${config.projectName}`);

      // æ„å»º AI æç¤ºè¯
      const prompt = this.buildReportPrompt(config);

      // è°ƒç”¨ AI API ç”Ÿæˆå†…å®¹
      const generatedText = await this.callAIAPI(prompt, config.reportType);

      // è§£æç”Ÿæˆçš„å†…å®¹
      const content = this.parseGeneratedContent(generatedText, config);

      console.log(`âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ: ${config.projectName}`);
      return content;

    } catch (error) {
      console.error('æŠ¥å‘Šç”Ÿæˆå¤±è´¥:', error);

      // è¿”å›æ¨¡æ‹Ÿç”Ÿæˆå†…å®¹
      return this.generateMockReportContent(config);
    }
  }

  /**
   * æå– PDF å†…å®¹
   */
  private async extractPdfContent(buffer: Buffer): Promise<string> {
    try {
      // è¿™é‡Œåº”è¯¥ä½¿ç”¨ pdf-parse æˆ–ç±»ä¼¼åº“
      // const pdf = require('pdf-parse');
      // const data = await pdf(buffer);
      // return data.text;

      // æ¨¡æ‹Ÿæå–çš„å†…å®¹
      return 'è¿™æ˜¯ä»PDFæ–‡ä»¶æå–çš„ç¤ºä¾‹å†…å®¹...';
    } catch (error) {
      console.error('PDFå†…å®¹æå–å¤±è´¥:', error);
      return 'æ— æ³•æå–PDFå†…å®¹';
    }
  }

  /**
   * æå– PPTX å†…å®¹
   */
  private async extractPptxContent(buffer: Buffer): Promise<string> {
    try {
      // è¿™é‡Œåº”è¯¥ä½¿ç”¨ç›¸åº”çš„åº“æ¥è§£æ PPTX æ–‡ä»¶
      // æ¨¡æ‹Ÿæå–çš„å†…å®¹
      return 'è¿™æ˜¯ä»PPTXæ–‡ä»¶æå–çš„ç¤ºä¾‹å†…å®¹...';
    } catch (error) {
      console.error('PPTXå†…å®¹æå–å¤±è´¥:', error);
      return 'æ— æ³•æå–PPTXå†…å®¹';
    }
  }

  /**
   * æ‰§è¡Œå†…å®¹åˆ†æ
   */
  private async performContentAnalysis(content: string, fileName: string): Promise<AIAnalysisResult> {
    const analysisPrompt = `
è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå¹¶æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼š

æ–‡ä»¶å: ${fileName}
å†…å®¹: ${content}

è¯·åˆ†æï¼š
1. æ–‡æ¡£çš„ä¸»è¦ç« èŠ‚ç»“æ„
2. å…³é”®è¦ç‚¹
3. å›¾è¡¨å’Œè§†è§‰å…ƒç´ çš„æ•°é‡
4. è®¾è®¡æ¨¡å¼ï¼ˆé¢œè‰²ã€å­—ä½“ã€å¸ƒå±€é£æ ¼ï¼‰
5. æ¨èçš„æŠ¥å‘Šæ¨¡æ¿ç±»å‹

è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
`;

    try {
      if (this.qwenApiKey) {
        return await this.callQwenAPI(analysisPrompt);
      } else {
        return this.generateMockAnalysisResult(fileName);
      }
    } catch (error) {
      console.error('AIå†…å®¹åˆ†æå¤±è´¥:', error);
      return this.generateMockAnalysisResult(fileName);
    }
  }

  /**
   * æ„å»ºæŠ¥å‘Šç”Ÿæˆæç¤ºè¯
   */
  private buildReportPrompt(config: ReportGenerationConfig): string {
    const { projectName, reportType, analysisArea, timeRange, additionalRequirements } = config;

    return `
è¯·ä¸ºæˆ¿åœ°äº§é¡¹ç›®ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„${reportType}ã€‚

é¡¹ç›®ä¿¡æ¯ï¼š
- é¡¹ç›®åç§°ï¼š${projectName}
- åˆ†æåŒºåŸŸï¼š${analysisArea}
- æ—¶é—´èŒƒå›´ï¼š${timeRange}
- ç‰¹æ®Šè¦æ±‚ï¼š${additionalRequirements || 'æ— '}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æŠ¥å‘Šï¼š
1. æ‰§è¡Œæ‘˜è¦
2. å¸‚åœºæ¦‚å†µ
3. è¯¦ç»†åˆ†æ
4. å…³é”®å‘ç°
5. å»ºè®®å’Œç»“è®º

è¦æ±‚ï¼š
- å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€æœ‰æ·±åº¦
- ç»“æ„æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
- åŒ…å«å…·ä½“çš„æ•°æ®åˆ†æ
- æä¾›å¯æ“ä½œçš„å»ºè®®
- è¯­è¨€ç®€æ´æ˜äº†

è¯·ä»¥ç»“æ„åŒ–çš„æ ¼å¼è¿”å›å†…å®¹ã€‚
`;
  }

  /**
   * è°ƒç”¨é€šä¹‰åƒé—® API
   */
  private async callQwenAPI(prompt: string): Promise<any> {
    try {
      const response = await fetch(`${this.baseURL}/services/aigc/text-generation/generation`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.qwenApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'qwen-turbo',
          input: {
            messages: [
              {
                role: 'user',
                content: prompt
              }
            ]
          }
        })
      });

      if (!response.ok) {
        throw new Error(`é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥: ${response.status}`);
      }

      const data = await response.json();
      return data.output.text;

    } catch (error) {
      console.error('é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * è°ƒç”¨ AI APIï¼ˆé€šç”¨ï¼‰
   */
  private async callAIAPI(prompt: string, context: string): Promise<string> {
    try {
      if (this.qwenApiKey) {
        return await this.callQwenAPI(prompt);
      } else if (this.openaiApiKey) {
        return await this.callOpenAIAPI(prompt);
      } else {
        // è¿”å›æ¨¡æ‹Ÿå†…å®¹
        return this.generateMockContent(context);
      }
    } catch (error) {
      console.error('AI APIè°ƒç”¨å¤±è´¥:', error);
      return this.generateMockContent(context);
    }
  }

  /**
   * è°ƒç”¨ OpenAI API
   */
  private async callOpenAIAPI(prompt: string): Promise<string> {
    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.openaiApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo',
          messages: [
            {
              role: 'user',
              content: prompt
            }
          ],
          max_tokens: 2000,
          temperature: 0.7
        })
      });

      if (!response.ok) {
        throw new Error(`OpenAI APIè°ƒç”¨å¤±è´¥: ${response.status}`);
      }

      const data = await response.json();
      return data.choices[0].message.content;

    } catch (error) {
      console.error('OpenAI APIè°ƒç”¨å¤±è´¥:', error);
      throw error;
    }
  }

  /**
   * è§£æç”Ÿæˆçš„å†…å®¹
   */
  private parseGeneratedContent(text: string, config: ReportGenerationConfig): GeneratedContent {
    // è¿™é‡Œåº”è¯¥æœ‰æ›´å¤æ‚çš„è§£æé€»è¾‘
    const sections: GeneratedSection[] = [
      {
        id: 'summary',
        title: 'æ‰§è¡Œæ‘˜è¦',
        content: 'æœ¬æŠ¥å‘Šå¯¹é¡¹ç›®è¿›è¡Œäº†å…¨é¢åˆ†æ...',
        type: 'text'
      },
      {
        id: 'market',
        title: 'å¸‚åœºæ¦‚å†µ',
        content: 'å½“å‰å¸‚åœºçŠ¶å†µæ˜¾ç¤º...',
        type: 'text'
      },
      {
        id: 'analysis',
        title: 'è¯¦ç»†åˆ†æ',
        content: 'é€šè¿‡æ·±å…¥åˆ†æå‘ç°...',
        type: 'text'
      }
    ];

    return {
      title: `${config.projectName} - ${config.reportType}`,
      sections,
      metadata: {
        generatedAt: new Date().toISOString(),
        model: this.qwenApiKey ? 'qwen-turbo' : 'gpt-3.5-turbo',
        confidence: 0.85,
        wordCount: text.length
      }
    };
  }

  /**
   * ç”Ÿæˆæ¨¡æ‹Ÿåˆ†æç»“æœ
   */
  private generateMockAnalysisResult(fileName: string): AIAnalysisResult {
    return {
      extractedContent: {
        sections: ['å°é¢', 'ç›®å½•', 'å¸‚åœºæ¦‚è§ˆ', 'ç«å“åˆ†æ', 'æ•°æ®åˆ†æ', 'ç»“è®ºä¸å»ºè®®'],
        keyPoints: [
          'å¸‚åœºéœ€æ±‚æŒç»­å¢é•¿',
          'ç«äº‰æ ¼å±€ç›¸å¯¹ç¨³å®š',
          'ä»·æ ¼è¶‹åŠ¿å‘ä¸Š',
          'æ”¿ç­–ç¯å¢ƒåˆ©å¥½'
        ],
        charts: 5,
        images: 8,
        tables: 3
      },
      designPatterns: {
        colorScheme: ['#2563eb', '#1d4ed8', '#1e40af', '#ffffff'],
        fonts: ['Microsoft YaHei', 'Arial', 'SimHei'],
        layout: 'professional',
        style: 'corporate'
      },
      suggestedTemplate: {
        reportType: 'å¸‚åœºåˆ†ææŠ¥å‘Š',
        estimatedTime: '12-15åˆ†é’Ÿ',
        requiredFields: ['é¡¹ç›®åç§°', 'åˆ†æåŒºåŸŸ', 'ç«å“é¡¹ç›®', 'æ—¶é—´èŒƒå›´'],
        outputFormat: ['pptx', 'pdf']
      }
    };
  }

  /**
   * ç”Ÿæˆæ¨¡æ‹ŸæŠ¥å‘Šå†…å®¹
   */
  private generateMockReportContent(config: ReportGenerationConfig): GeneratedContent {
    return {
      title: `${config.projectName} - ${config.reportType}`,
      sections: [
        {
          id: 'summary',
          title: 'æ‰§è¡Œæ‘˜è¦',
          content: `æœ¬æŠ¥å‘Šé’ˆå¯¹${config.projectName}é¡¹ç›®è¿›è¡Œäº†å…¨é¢çš„å¸‚åœºåˆ†æã€‚é€šè¿‡å¯¹${config.analysisArea}åŒºåŸŸ${config.timeRange}çš„æ•°æ®åˆ†æï¼Œæˆ‘ä»¬å‘ç°è¯¥é¡¹ç›®å…·æœ‰è‰¯å¥½çš„å¸‚åœºå‰æ™¯å’ŒæŠ•èµ„ä»·å€¼ã€‚ä¸»è¦ç»“è®ºåŒ…æ‹¬ï¼šå¸‚åœºéœ€æ±‚ç¨³å®šå¢é•¿ï¼Œç«äº‰ç¯å¢ƒç›¸å¯¹æ¸©å’Œï¼Œå®šä»·ç­–ç•¥åˆç†ï¼Œé¢„æœŸæ”¶ç›Šç‡è¾¾åˆ°é¢„æœŸæ°´å¹³ã€‚`,
          type: 'text'
        },
        {
          id: 'market_overview',
          title: 'å¸‚åœºæ¦‚å†µ',
          content: `${config.analysisArea}åœ°åŒºæˆ¿åœ°äº§å¸‚åœºåœ¨${config.timeRange}è¡¨ç°ç¨³å¥ã€‚åŒºåŸŸå†…æ–°æˆ¿ä¾›åº”é‡é€‚ä¸­ï¼ŒäºŒæ‰‹æˆ¿äº¤æ˜“æ´»è·ƒï¼Œæ•´ä½“ä»·æ ¼å‘ˆç°ç¨³ä¸­æœ‰å‡çš„æ€åŠ¿ã€‚æ”¿ç­–ç¯å¢ƒæ€»ä½“å‘å¥½ï¼ŒåŸºç¡€è®¾æ–½å»ºè®¾æŒç»­å®Œå–„ï¼Œä¸ºæˆ¿åœ°äº§å¸‚åœºå‘å±•æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚`,
          type: 'text'
        },
        {
          id: 'competitive_analysis',
          title: 'ç«å“åˆ†æ',
          content: `é€šè¿‡å¯¹åŒºåŸŸå†…ä¸»è¦ç«äº‰é¡¹ç›®çš„åˆ†æï¼Œ${config.projectName}åœ¨äº§å“å®šä½ã€ä»·æ ¼ç­–ç•¥ã€è¥é”€æ¨å¹¿ç­‰æ–¹é¢å…·æœ‰ä¸€å®šä¼˜åŠ¿ã€‚å»ºè®®åŠ å¼ºå·®å¼‚åŒ–ç«äº‰ï¼Œçªå‡ºé¡¹ç›®ç‰¹è‰²ï¼Œæå‡å“ç‰ŒçŸ¥ååº¦ã€‚`,
          type: 'text'
        },
        {
          id: 'recommendations',
          title: 'å»ºè®®ä¸ç»“è®º',
          content: `åŸºäºä»¥ä¸Šåˆ†æï¼Œæˆ‘ä»¬å»ºè®®ï¼š1ï¼‰æŠŠæ¡å¸‚åœºæ—¶æœºï¼ŒåŠ å¿«é¡¹ç›®æ¨è¿›ï¼›2ï¼‰ä¼˜åŒ–äº§å“è®¾è®¡ï¼Œæ»¡è¶³ç›®æ ‡å®¢ç¾¤éœ€æ±‚ï¼›3ï¼‰åˆ¶å®šåˆç†çš„å®šä»·ç­–ç•¥ï¼›4ï¼‰åŠ å¼ºè¥é”€æ¨å¹¿ï¼Œæå‡å¸‚åœºè®¤çŸ¥åº¦ã€‚æ€»ä½“è€Œè¨€ï¼Œ${config.projectName}é¡¹ç›®å…·æœ‰è‰¯å¥½çš„å¸‚åœºå‰æ™¯å’ŒæŠ•èµ„ä»·å€¼ã€‚`,
          type: 'text'
        }
      ],
      metadata: {
        generatedAt: new Date().toISOString(),
        model: 'mock',
        confidence: 0.9,
        wordCount: 800
      }
    };
  }

  /**
   * ç”Ÿæˆæ¨¡æ‹Ÿå†…å®¹
   */
  private generateMockContent(context: string): string {
    return `è¿™æ˜¯ä¸º${context}ç”Ÿæˆçš„æ¨¡æ‹Ÿå†…å®¹ã€‚åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯ç”±AIæ¨¡å‹ç”Ÿæˆçš„ä¸“ä¸šå†…å®¹ã€‚`;
  }

  /**
   * è°ƒç”¨å…·ä½“çš„ AI æä¾›å•†
   */
  private async callAIProvider(
    provider: AIProviderConfig,
    prompt: string,
    request: AIRequest
  ): Promise<AIResponse> {
    switch (provider.name) {
      case 'qwen':
        return this.callQwenAPIEnhanced(provider, prompt, request);
      case 'openai':
        return this.callOpenAIAPIEnhanced(provider, prompt, request);
      case 'mock':
        return this.callMockAPIEnhanced(provider, prompt, request);
      default:
        throw new Error(`ä¸æ”¯æŒçš„ AI æä¾›å•†: ${provider.name}`);
    }
  }

  /**
   * å¢å¼ºç‰ˆé€šä¹‰åƒé—® API è°ƒç”¨
   */
  private async callQwenAPIEnhanced(
    provider: AIProviderConfig,
    prompt: string,
    request: AIRequest
  ): Promise<AIResponse> {
    const requestBody = {
      model: provider.model,
      input: {
        messages: [
          {
            role: 'system',
            content: this.buildSystemPrompt(request.requestType)
          },
          {
            role: 'user',
            content: prompt
          }
        ]
      },
      parameters: {
        max_tokens: request.maxTokens || provider.maxTokens,
        temperature: request.temperature || 0.7,
        result_format: 'message'
      }
    };

    const response = await fetch(provider.endpoint!, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${provider.apiKey}`
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      throw new Error(`é€šä¹‰åƒé—® API è°ƒç”¨å¤±è´¥: ${response.statusText}`);
    }

    const data = await response.json();

    if (data.code) {
      throw new Error(`é€šä¹‰åƒé—® API é”™è¯¯: ${data.message}`);
    }

    const content = data.output.choices[0].message.content;
    const usage = data.usage;

    return {
      content,
      usage: {
        promptTokens: usage.input_tokens,
        completionTokens: usage.output_tokens,
        totalTokens: usage.total_tokens
      },
      model: provider.model,
      provider: 'qwen',
      cached: false
    };
  }

  /**
   * å¢å¼ºç‰ˆ OpenAI API è°ƒç”¨
   */
  private async callOpenAIAPIEnhanced(
    provider: AIProviderConfig,
    prompt: string,
    request: AIRequest
  ): Promise<AIResponse> {
    const requestBody = {
      model: provider.model,
      messages: [
        {
          role: 'system',
          content: this.buildSystemPrompt(request.requestType)
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      max_tokens: request.maxTokens || provider.maxTokens,
      temperature: request.temperature || 0.7
    };

    const response = await fetch(provider.endpoint!, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${provider.apiKey}`
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      throw new Error(`OpenAI API è°ƒç”¨å¤±è´¥: ${response.statusText}`);
    }

    const data = await response.json();

    if (data.error) {
      throw new Error(`OpenAI API é”™è¯¯: ${data.error.message}`);
    }

    const content = data.choices[0].message.content;
    const usage = data.usage;

    return {
      content,
      usage: {
        promptTokens: usage.prompt_tokens,
        completionTokens: usage.completion_tokens,
        totalTokens: usage.total_tokens
      },
      model: provider.model,
      provider: 'openai',
      cached: false
    };
  }

  /**
   * å¢å¼ºç‰ˆ Mock API è°ƒç”¨
   */
  private async callMockAPIEnhanced(
    provider: AIProviderConfig,
    prompt: string,
    request: AIRequest
  ): Promise<AIResponse> {
    // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000));

    const mockResponses = this.getMockResponses(request.requestType);
    const randomResponse = mockResponses[Math.floor(Math.random() * mockResponses.length)];

    const promptTokens = Math.ceil(prompt.length / 4);
    const completionTokens = Math.ceil(randomResponse.length / 4);

    return {
      content: randomResponse,
      usage: {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens
      },
      model: provider.model,
      provider: 'mock',
      cached: false
    };
  }

  /**
   * é€‰æ‹©æœ€ä¼˜çš„ AI æä¾›å•†
   */
  private selectProvider(request: AIRequest): AIProviderConfig | null {
    const availableProviders = Array.from(this.providers.values())
      .filter(provider => provider.enabled)
      .sort((a, b) => a.priority - b.priority);

    for (const provider of availableProviders) {
      if (this.isProviderSuitable(provider, request)) {
        return provider;
      }
    }

    return availableProviders[0] || null;
  }

  /**
   * æ£€æŸ¥æä¾›å•†æ˜¯å¦é€‚åˆå½“å‰è¯·æ±‚
   */
  private isProviderSuitable(provider: AIProviderConfig, request: AIRequest): boolean {
    const estimatedTokens = Math.ceil(request.prompt.length / 4) + (request.maxTokens || 1000);
    if (estimatedTokens > provider.maxTokens) {
      return false;
    }

    if (request.requestType === 'analysis' && provider.name === 'mock') {
      return false;
    }

    return true;
  }

  /**
   * æ„å»ºç³»ç»Ÿæç¤ºè¯
   */
  private buildSystemPrompt(requestType: string): string {
    const prompts = {
      generation: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æˆ¿åœ°äº§è¥é”€æŠ¥å‘Šç”ŸæˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯ç”Ÿæˆé«˜è´¨é‡çš„è¥é”€æŠ¥å‘Šå†…å®¹ã€‚',
      analysis: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚è¯·åˆ†æç”¨æˆ·æä¾›çš„å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç»“æ„åŒ–çš„åˆ†æç»“æœã€‚',
      chat: 'ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£å†³æˆ¿åœ°äº§è¥é”€ç›¸å…³é—®é¢˜ã€‚',
      summary: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚è¯·ä¸ºç”¨æˆ·æä¾›çš„å†…å®¹ç”Ÿæˆç®€æ´å‡†ç¡®çš„æ‘˜è¦ã€‚'
    };

    return prompts[requestType as keyof typeof prompts] || prompts.chat;
  }

  /**
   * å¤„ç†æœåŠ¡é™çº§
   */
  private async handleServiceDegradation(
    request: AIRequest,
    degradationInfo: any
  ): Promise<AIResponse> {
    switch (degradationInfo.degradeToLevel) {
      case 'cache':
        const cacheKey = this.generateCacheKey(request);
        const cached = this.responseCache.get(cacheKey);
        if (cached) return cached;
        return this.getStaticResponse(request);

      case 'simple':
        return this.getSimpleResponse(request);

      case 'offline':
        throw new Error('AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•');

      default:
        throw new Error('æœªçŸ¥çš„é™çº§ç­–ç•¥');
    }
  }

  /**
   * ç”Ÿæˆç¼“å­˜é”®
   */
  private generateCacheKey(request: AIRequest): string {
    const key = `${request.requestType}:${request.prompt}:${request.maxTokens || 'default'}`;
    return crypto.createHash('md5').update(key).digest('hex');
  }

  /**
   * æ¸…ç†è¿‡æœŸç¼“å­˜
   */
  private cleanupCache() {
    if (this.responseCache.size > 1000) {
      const entries = Array.from(this.responseCache.entries());
      const halfSize = Math.floor(entries.length / 2);
      this.responseCache.clear();

      for (let i = halfSize; i < entries.length; i++) {
        this.responseCache.set(entries[i][0], entries[i][1]);
      }
    }
  }

  /**
   * è·å–é™æ€å“åº”ï¼ˆé™çº§ç­–ç•¥ï¼‰
   */
  private getStaticResponse(request: AIRequest): AIResponse {
    const content = `ç³»ç»Ÿæ­£åœ¨ç»´æŠ¤ä¸­ï¼Œè¿™æ˜¯é’ˆå¯¹"${request.requestType}"è¯·æ±‚çš„é¢„è®¾å“åº”ã€‚è¯·ç¨åé‡è¯•ä»¥è·å–æ›´å‡†ç¡®çš„ç»“æœã€‚`;

    return {
      content,
      usage: {
        promptTokens: 0,
        completionTokens: content.length / 4,
        totalTokens: content.length / 4
      },
      model: 'static',
      provider: 'mock',
      cached: false
    };
  }

  /**
   * è·å–ç®€åŒ–å“åº”ï¼ˆé™çº§ç­–ç•¥ï¼‰
   */
  private getSimpleResponse(request: AIRequest): AIResponse {
    const responses = this.getMockResponses(request.requestType);
    const content = responses[0];

    return {
      content,
      usage: {
        promptTokens: Math.ceil(request.prompt.length / 4),
        completionTokens: Math.ceil(content.length / 4),
        totalTokens: Math.ceil((request.prompt.length + content.length) / 4)
      },
      model: 'simple',
      provider: 'mock',
      cached: false
    };
  }

  /**
   * è·å–Mockå“åº”æ¨¡æ¿
   */
  private getMockResponses(requestType: string): string[] {
    const responses = {
      generation: [
        '# æˆ¿åœ°äº§è¥é”€æŠ¥å‘Š\n\n## é¡¹ç›®æ¦‚è¿°\nåŸºäºæ‚¨æä¾›çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¸ºè¯¥æˆ¿åœ°äº§é¡¹ç›®åˆ¶å®šäº†å…¨é¢çš„è¥é”€æ–¹æ¡ˆã€‚\n\n## æ ¸å¿ƒäº®ç‚¹\n- åœ°ç†ä½ç½®ä¼˜è¶Š\n- é…å¥—è®¾æ–½å®Œå–„\n- æŠ•èµ„æ½œåŠ›å·¨å¤§\n\n## è¥é”€å»ºè®®\nå»ºè®®é‡‡ç”¨å¤šæ¸ é“è¥é”€ç­–ç•¥ï¼Œé‡ç‚¹çªå‡ºé¡¹ç›®æ ¸å¿ƒç«äº‰ä¼˜åŠ¿ã€‚',
        '# å¸‚åœºåˆ†ææŠ¥å‘Š\n\n## å¸‚åœºç°çŠ¶\nå½“å‰æˆ¿åœ°äº§å¸‚åœºå‘ˆç°ç¨³ä¸­æœ‰å‡çš„æ€åŠ¿ã€‚\n\n## ç«äº‰åˆ†æ\nä¸»è¦ç«äº‰å¯¹æ‰‹åˆ†ææ˜¾ç¤ºï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼š\n- ä»·æ ¼ç«äº‰åŠ›\n- äº§å“å·®å¼‚åŒ–\n- æœåŠ¡è´¨é‡\n\n## æŠ•èµ„å»ºè®®\nå»ºè®®æŠŠæ¡å¸‚åœºæœºé‡ï¼Œé€‚æ—¶æ¨å‡ºè¥é”€æ´»åŠ¨ã€‚'
      ],
      analysis: [
        '## åˆ†æç»“æœ\n\n### å…³é”®æ•°æ®ç‚¹\n- æ•°æ®å®Œæ•´æ€§ï¼š95%\n- è¶‹åŠ¿æ–¹å‘ï¼šä¸Šå‡\n- é£é™©è¯„çº§ï¼šä¸­ç­‰\n\n### ä¸»è¦å‘ç°\n1. å¸‚åœºéœ€æ±‚æŒç»­å¢é•¿\n2. ä»·æ ¼æ³¢åŠ¨åœ¨åˆç†èŒƒå›´å†…\n3. åŒºåŸŸå‘å±•å‰æ™¯è‰¯å¥½\n\n### å»ºè®®è¡ŒåŠ¨\nå»ºè®®ç»§ç»­å…³æ³¨å¸‚åœºåŠ¨æ€ï¼Œé€‚æ—¶è°ƒæ•´ç­–ç•¥ã€‚'
      ],
      chat: [
        'æ‚¨å¥½ï¼æˆ‘æ˜¯MarketPro AIåŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºæ‚¨æä¾›æˆ¿åœ°äº§è¥é”€ç›¸å…³çš„ä¸“ä¸šå»ºè®®ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ'
      ],
      summary: [
        '## å†…å®¹æ‘˜è¦\n\næœ¬æ–‡æ¡£åŒ…å«äº†è¯¦ç»†çš„é¡¹ç›®ä¿¡æ¯å’Œå¸‚åœºåˆ†ææ•°æ®ã€‚ä¸»è¦å†…å®¹æ¶µç›–é¡¹ç›®ç‰¹è‰²ã€ç›®æ ‡å®¢ç¾¤ã€è¥é”€ç­–ç•¥ç­‰å…³é”®è¦ç´ ã€‚å»ºè®®é‡ç‚¹å…³æ³¨å·®å¼‚åŒ–ç«äº‰ä¼˜åŠ¿çš„å±•ç¤ºã€‚'
      ]
    };

    return responses[requestType as keyof typeof responses] || responses.chat;
  }

  /**
   * è·å–ä½¿ç”¨ç»Ÿè®¡
   */
  async getUsageStats(userId?: string, timeRange: 'hour' | 'day' | 'month' = 'day') {
    if (userId) {
      return this.securityService.getUserUsageStats(userId, timeRange);
    }

    // å…¨å±€ç»Ÿè®¡
    const now = new Date();
    let startTime: Date;

    switch (timeRange) {
      case 'hour':
        startTime = new Date(now.getTime() - 60 * 60 * 1000);
        break;
      case 'day':
        startTime = new Date(now.getTime() - 24 * 60 * 60 * 1000);
        break;
      case 'month':
        startTime = new Date(now.getTime() - 30 * 24 * 60 * 60 * 1000);
        break;
    }

    const stats = await this.prisma.aIUsageLog.aggregate({
      where: {
        timestamp: { gte: startTime }
      },
      _sum: {
        totalTokens: true,
        estimatedCost: true
      },
      _count: { id: true }
    });

    return {
      period: timeRange,
      startTime,
      endTime: now,
      totalRequests: stats._count.id,
      totalTokens: stats._sum.totalTokens || 0,
      totalCost: stats._sum.estimatedCost || 0,
      cacheHitRate: this.calculateCacheHitRate()
    };
  }

  /**
   * è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
   */
  private calculateCacheHitRate(): number {
    return Math.min(0.8, this.responseCache.size / 100);
  }

  /**
   * å¥åº·æ£€æŸ¥
   */
  async healthCheck(): Promise<{
    status: 'healthy' | 'degraded' | 'critical';
    providers: Record<AIProvider, boolean>;
    cacheSize: number;
    securityStatus: any;
    legacy: {
      status: 'healthy' | 'degraded' | 'error';
      services: {
        qwen: 'available' | 'unavailable' | 'not_configured';
        openai: 'available' | 'unavailable' | 'not_configured';
      };
    };
  }> {
    const providerStatus: Record<AIProvider, boolean> = {
      qwen: false,
      openai: false,
      mock: true
    };

    // æ£€æŸ¥å„ä¸ªæä¾›å•†çš„çŠ¶æ€
    for (const [name, config] of this.providers.entries()) {
      if (config.enabled && config.apiKey && config.apiKey !== 'mock-key') {
        providerStatus[name] = true;
      }
    }

    const securityStatus = await this.securityService.healthCheck();

    const hasWorkingProvider = Object.values(providerStatus).some(status => status);
    const overallStatus = !hasWorkingProvider ? 'critical' :
                         securityStatus.status === 'critical' ? 'degraded' : 'healthy';

    // Legacy format for backward compatibility
    const legacyServices = {
      qwen: this.qwenApiKey ? 'available' as const : 'not_configured' as const,
      openai: this.openaiApiKey ? 'available' as const : 'not_configured' as const
    };

    const legacyHasAnyAPI = legacyServices.qwen === 'available' || legacyServices.openai === 'available';
    const legacyStatus = legacyHasAnyAPI ? 'healthy' as const : 'degraded' as const;

    return {
      status: overallStatus,
      providers: providerStatus,
      cacheSize: this.responseCache.size,
      securityStatus,
      legacy: {
        status: legacyStatus,
        services: legacyServices
      }
    };
  }
}